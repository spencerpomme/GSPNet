{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo_city.png\" align=\"right\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Graph State Prediction Network (GSPNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a prototype on how to train a graph state prediction network using CNN and LSTM.\n",
    "The content is devided into __3__ parts:\n",
    "\n",
    "1. Data preprocessing\n",
    "2. Model building, Training and Tuning\n",
    "3. Prediction and per demand modifacation\n",
    "\n",
    "The model is built with [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries:\n",
    "\n",
    "We will use pandas and numpy as main data examine and cleansing util and use dask to handle parallel computing cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlalchemy\n",
    "import dask\n",
    "import time\n",
    "import psycopg2\n",
    "import warnings\n",
    "import re\n",
    "import torch\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br></br>\n",
    "<br></br>\n",
    "We'll first, use a small sample data (approximately 1 million rows) and see a few rows of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load time is 42.912627.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripid</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>trip_avg_speed</th>\n",
       "      <th>trip_time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35972297</td>\n",
       "      <td>2017-03-09 21:30:11</td>\n",
       "      <td>2017-03-09 21:44:20</td>\n",
       "      <td>148</td>\n",
       "      <td>48</td>\n",
       "      <td>4.06</td>\n",
       "      <td>1</td>\n",
       "      <td>18.36</td>\n",
       "      <td>00:14:09</td>\n",
       "      <td>17.215548</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35972298</td>\n",
       "      <td>2017-03-09 21:47:00</td>\n",
       "      <td>2017-03-09 21:58:01</td>\n",
       "      <td>48</td>\n",
       "      <td>107</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "      <td>12.80</td>\n",
       "      <td>00:11:01</td>\n",
       "      <td>14.868381</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35972299</td>\n",
       "      <td>2017-03-09 22:01:08</td>\n",
       "      <td>2017-03-09 22:11:16</td>\n",
       "      <td>79</td>\n",
       "      <td>162</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1</td>\n",
       "      <td>14.12</td>\n",
       "      <td>00:10:08</td>\n",
       "      <td>13.440789</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35972300</td>\n",
       "      <td>2017-03-09 22:16:05</td>\n",
       "      <td>2017-03-10 06:26:11</td>\n",
       "      <td>237</td>\n",
       "      <td>41</td>\n",
       "      <td>3.86</td>\n",
       "      <td>1</td>\n",
       "      <td>17.29</td>\n",
       "      <td>08:10:06</td>\n",
       "      <td>0.472557</td>\n",
       "      <td>29406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35972301</td>\n",
       "      <td>2017-03-31 06:31:53</td>\n",
       "      <td>2017-03-31 06:41:48</td>\n",
       "      <td>41</td>\n",
       "      <td>162</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1</td>\n",
       "      <td>13.30</td>\n",
       "      <td>00:09:55</td>\n",
       "      <td>20.873950</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tripid tpep_pickup_datetime tpep_dropoff_datetime  pulocationid  \\\n",
       "0  35972297  2017-03-09 21:30:11   2017-03-09 21:44:20           148   \n",
       "1  35972298  2017-03-09 21:47:00   2017-03-09 21:58:01            48   \n",
       "2  35972299  2017-03-09 22:01:08   2017-03-09 22:11:16            79   \n",
       "3  35972300  2017-03-09 22:16:05   2017-03-10 06:26:11           237   \n",
       "4  35972301  2017-03-31 06:31:53   2017-03-31 06:41:48            41   \n",
       "\n",
       "   dolocationid  trip_distance  passenger_count  total_amount trip_time  \\\n",
       "0            48           4.06                1         18.36  00:14:09   \n",
       "1           107           2.73                1         12.80  00:11:01   \n",
       "2           162           2.27                1         14.12  00:10:08   \n",
       "3            41           3.86                1         17.29  08:10:06   \n",
       "4           162           3.45                1         13.30  00:09:55   \n",
       "\n",
       "   trip_avg_speed  trip_time_sec  \n",
       "0       17.215548            849  \n",
       "1       14.868381            661  \n",
       "2       13.440789            608  \n",
       "3        0.472557          29406  \n",
       "4       20.873950            595  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load already cleaned data sample using plain pandas\n",
    "t1 = time.time()\n",
    "table = pd.read_csv('dataset/nytaxi_yellow_2017_mar.csv')\n",
    "print(f'Load time is {time.time() - t1 :2f}.')\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. First sight of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the the type of columns and some basic statistics of the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column data types\n",
    "table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows and columns\n",
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic statistics\n",
    "table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we will use are: **tpep_pickup_datetime, pulocationid, tpep_dropoff_datetime, dolocationid.**\n",
    "These information are used to generate graphs that are used to discribe the general traffic state of New York.\n",
    "\n",
    "Notice currently the **tpep_pickup_datetime** and **tpep_dropoff_datetime** are of type `object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.loc[:,['tpep_pickup_datetime', 'tpep_dropoff_datetime']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do **time slice** later, these two feilds must be sorted in chronological mananer, first by `tpep_pickup_datetime` and then by `tpep_dropoff_datetime`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick first 10 rows\n",
    "# not supported for dask DataFrame\n",
    "table.sort_values(by=['tpep_pickup_datetime', 'tpep_dropoff_datetime']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that the taxi trips are nicely sorted according to **time**. What we need to do is convert this tabular date into graphs that described by **adjacency matrice**.\n",
    "\n",
    "First, let's see what are `pulocationid` and `dolocationid`:\n",
    "\n",
    "<img src=\"images/map.jpg\" align=\"center\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are code number for a district in Manhattan, New York. We will mainly describe the traffic state and prediction at **this** level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Time slicing\n",
    "\n",
    "Now we've done a very superficial exploration of the data. It's time to convert the tabular data to graphs that are to be fed into our neural network model.\n",
    "\n",
    "The first problem is, **how to decide a feasible time interval?**\n",
    "\n",
    "Of course, the time interval should be a varaible that is to be selected **per need. However,** it is a reasonable thinking to let a time interval (i.e. a **snapshot**) to contain as many as **complete trip** as possible. See the diagram below:\n",
    "\n",
    "<img src=\"images/snapshot.png\" align=\"center\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **If the time interval is too small**:\n",
    "\n",
    "The majority of the matrices will be very sparse or even completely blank.\n",
    "\n",
    "#### If the time interval is too big:\n",
    "\n",
    "Then information about trips will be densely squeezed into one matrix and a lot of details on changes along time will be lost.\n",
    "\n",
    "Thus, it is important to choose time interval wisely. Let's look at the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.loc[:,['trip_distance', 'trip_time_sec', 'trip_avg_speed']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The an average trip lasts for roughly 900 seconds, which is **15 minutes**.\n",
    "\n",
    "The median(i.e. 50% percentile) is roughly 600 seconds, which is **10 minutes**.\n",
    "\n",
    "Let's plot the distribution of column `trip_time_sec` to have a better intuition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "sns.set(color_codes=True)\n",
    "sns.distplot(table.loc[:,'trip_time_sec']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the trips are mostly concentrated in a shorter time range, while some outliers may significantly biased the average.\n",
    "\n",
    "Let's see how how many trips are longer than 30 minutes (1800 secs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with column 'trip_time_sec' value larger than 1800 divided by total number of rows\n",
    "table.loc[table['trip_time_sec'] > 1800].shape[0] / table.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask version of above cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **95%** of the trips last shorter than 30 minutes, let's plot distribution at this range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution of 95% trip data in time duration\n",
    "temp = table.loc[table['trip_time_sec'] <= 1800]\n",
    "sns.distplot(temp.loc[:,'trip_time_sec']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# statistics on the 95% data\n",
    "temp.loc[:,['trip_distance', 'trip_time_sec', 'trip_avg_speed']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value and their appearance\n",
    "pd.DataFrame(temp.loc[:,'trip_time_sec'].value_counts().sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 50% of the trips have trip time between 6 to 15 minutes, with a median of 10 minutes.\n",
    "\n",
    "#### As a result, we primarily decide to use 10 minutes as time interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. From tabular data to graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have decided a time interval, it's time to generate matrices. Basically, the idea is shown as figure below:\n",
    "\n",
    "<img src=\"images/matrices.png\" align=\"left\" width=\"85%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from **outgoin** and **incoming** matrices shown above, an additional **Domestic** matrix is add because 50% of trips last less than the **chosen interval 10 minutes**. Such matrices can be combined into a **Tensor** illustrated as follows:\n",
    "\n",
    "<img src=\"images/layers.png\" align=\"center\" width=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STRICT DEFINITION OF OID layers\n",
    "\n",
    "+ **F**uture layer\n",
    "    Condition: \n",
    "\n",
    "    1. left bound of interval  <= tpep_pickup_datetime  < right bound of interval\n",
    "    2. right bound of interval <= tpep_dropoff_datetime\n",
    "\n",
    "\n",
    "+ **P**ast layer\n",
    "    Condition: \n",
    "\n",
    "    1. left bound of interval  <= tpep_dropoff_datetime  < right bound of interval\n",
    "    2. tpep_pickup_datetime < left bound of interval\n",
    "\n",
    "\n",
    "+ **N**ow layer\n",
    "    Condition: \n",
    "\n",
    "    1. left bound of interval  <= tpep_pickup_datetime < tpep_dropoff_datetime  < right bound of interval\n",
    "\n",
    "### Notice: The PNF layers are redefined and renamed from OID layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract relevant columns:\n",
    "\n",
    "To generate tensor defined above, **four** key columns are needed.They are `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `pulocationid` and `dolocationid`.\n",
    "\n",
    "We first extract them from original table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw four needed columns and a id column, then sort according to time.\n",
    "# the `tripid` column is for the sake of naming.\n",
    "tensor_gen_f = table.loc[:,['tripid',\n",
    "                          'tpep_pickup_datetime',\n",
    "                          'tpep_dropoff_datetime',\n",
    "                          'pulocationid', 'dolocationid']\n",
    "                      ].sort_values(by=['tpep_pickup_datetime', 'tpep_dropoff_datetime']) # the first sort condition rules\n",
    "tensor_gen_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice the table to sub-tables according to time interval (10 min)\n",
    "\n",
    "Since the columns are extracted, now it's time to generate tensors.\n",
    "\n",
    "The dataset is trip data of all yellow taxi cabs in Manhattan area in **January**, 2017.As we are making 10-minute-splices, there will be total:\n",
    "\n",
    "$$\n",
    "6 \\times 24 \\times 31 = 4464 \\space slices(i.e. \\space snapshots)\n",
    "$$\n",
    "\n",
    "Each slice has 3 layers, that is in total\n",
    "\n",
    "$$\n",
    "4464 \\times 3 = 13392 \\space matrices\n",
    "$$\n",
    "to be generated. \n",
    "\n",
    "First, let's try to generate one image (i.e. a *tensor* with **3** layers).\n",
    "\n",
    "The time columns need to be converted to pandas timestamp type in order to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current type is python str\n",
    "type(tensor_gen_f.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to timestamp\n",
    "pd.to_datetime(tensor_gen_f.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.to_datetime(tensor_gen_f.iloc[0,1]) # 2017-01-01 00:00:00\n",
    "t2 = pd.to_datetime(tensor_gen_f.iloc[0,2]) # 2017-01-01 00:01:00\n",
    "t1 < t2 # test if can compare time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply type cast to all values:\n",
    "tensor_gen_f['tpep_pickup_datetime'] = pd.to_datetime(tensor_gen_f['tpep_pickup_datetime'])\n",
    "tensor_gen_f['tpep_dropoff_datetime'] = pd.to_datetime(tensor_gen_f['tpep_dropoff_datetime'])\n",
    "\n",
    "tensor_gen_f.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_gen_f.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interval **boundaries** are fixed when interval is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create intervals\n",
    "intervals = pd.date_range('2017-04-01 00:00:00', '2017-05-01 00:00:00', freq='10min')\n",
    "print(len(intervals))\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**:\n",
    "\n",
    "When slicing time interval in such manner, we are assuming the **entire** month is monitored. Under this setting, we do not consider another day as a 'fresh start'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(intervals[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First sub table:\n",
    "first = tensor_gen_f.loc[tensor_gen_f['tpep_pickup_datetime'] < intervals[1]]\n",
    "print(f'shape is: {first.shape}')\n",
    "first.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort to have a clearer look:\n",
    "first.sort_values(by=['pulocationid', 'dolocationid']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `first` will generate Outgoing, Incoming and Domestic layers, that is, 3 matrices of size: \n",
    "\n",
    "$$\n",
    "(number \\space of\\space zones)\\space \\times \\space (number\\space of\\space zones)\n",
    "$$\n",
    "\n",
    "Let's see the zone numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zone lookup table\n",
    "zones = pd.read_csv('dataset/taxi_zone_lookup.csv')\n",
    "print(f'shape is: {zones.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 265 zones, indexing from 0 to 264.\n",
    "\n",
    "Among which, we only care about `Yellow Zone` or `Manhattan` area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_zone = zones.loc[zones['Borough'] == 'Manhattan']\n",
    "print(y_zone.shape)\n",
    "img_size = y_zone.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the layers should be matrices of size (69, 69), and each timeslice(snapshot) should be tensors of size (69, 69, 3).\n",
    "\n",
    "Create the first tensor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The key is to use generated intervals to 'catch' trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legacy code, currently in use\n",
    "left = 1481\n",
    "right = 1482\n",
    "\n",
    "# convert dtype for entire table here:\n",
    "table['tpep_pickup_datetime'] = pd.to_datetime(table['tpep_pickup_datetime'])\n",
    "table['tpep_dropoff_datetime'] = pd.to_datetime(table['tpep_dropoff_datetime'])\n",
    "\n",
    "sorted_table = table.loc[:,['tripid',\n",
    "                          'tpep_pickup_datetime',\n",
    "                          'tpep_dropoff_datetime',\n",
    "                          'pulocationid', 'dolocationid']\n",
    "                        ].sort_values(by=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "\n",
    "# The condition of making snapshot should be: at least one temporal end of a trip should be within the bounds:\n",
    "snap = sorted_table.loc[((sorted_table['tpep_pickup_datetime'] >= intervals[left]) &\n",
    "                        (sorted_table['tpep_pickup_datetime'] < intervals[right])) |\n",
    "                        ((sorted_table['tpep_dropoff_datetime'] >= intervals[left]) &\n",
    "                        (sorted_table['tpep_dropoff_datetime'] < intervals[right]))\n",
    "                        ]\n",
    "print(f'sorted_table.shape -> {sorted_table.shape}')\n",
    "print(f'snap.shape -> {snap.shape}')\n",
    "\n",
    "# temp table to generate F,P,N layers\n",
    "# No need to sort\n",
    "temp_snap = snap.loc[:,['tripid',\n",
    "                          'tpep_pickup_datetime',\n",
    "                          'tpep_dropoff_datetime',\n",
    "                          'pulocationid', 'dolocationid']\n",
    "                    ]\n",
    "\n",
    "\n",
    "# Use the interval to 'catch' trips.\n",
    "f_flayer = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] < intervals[right]) &\n",
    "                            (temp_snap['tpep_pickup_datetime'] >= intervals[left]) &\n",
    "                            (temp_snap['tpep_dropoff_datetime'] >= intervals[right])\n",
    "                           ]\n",
    "\n",
    "f_player = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] < intervals[left]) &\n",
    "                            (temp_snap['tpep_dropoff_datetime'] >= intervals[left]) &\n",
    "                            (temp_snap['tpep_dropoff_datetime'] < intervals[right])\n",
    "                           ]\n",
    "\n",
    "f_nlayer = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] >= intervals[left]) &\n",
    "                            (temp_snap['tpep_dropoff_datetime'] < intervals[right])\n",
    "                           ]\n",
    "\n",
    "print(f'f_flayer.shape: {f_flayer.shape}')\n",
    "print(f'f_player.shape: {f_player.shape}') # There will be no incoming trips for the first snapshot.\n",
    "print(f'f_nlayer.shape: {f_nlayer.shape}')\n",
    "\n",
    "# Their shape should be the same\n",
    "assert temp_snap.shape[0] == f_flayer.shape[0] + f_player.shape[0] + f_nlayer.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the above procedure into a function for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_snap_layers(table, bound):\n",
    "    '''\n",
    "    Generate Past layer, Now layer and Future layer for one snapshot.\n",
    "    Params:\n",
    "        table:\n",
    "        bounds:\n",
    "    Return:\n",
    "        PNF layers, a list.\n",
    "    '''\n",
    "    # left bound and right bound of time interval\n",
    "    assert type(bound) == tuple\n",
    "    left = bound[0]\n",
    "    right = bound[1]\n",
    "    \n",
    "    print(left, right)\n",
    "\n",
    "    sorted_table = table.loc[:, ['tripid',\n",
    "                                 'tpep_pickup_datetime',\n",
    "                                 'tpep_dropoff_datetime',\n",
    "                                 'pulocationid',\n",
    "                                 'dolocationid']].sort_values(by=['tpep_pickup_datetime',\n",
    "                                                                  'tpep_dropoff_datetime'])\n",
    "\n",
    "    # The condition of making snapshot should be:\n",
    "    # at least one temporal end of a trip should be within the bounds:\n",
    "    snap = sorted_table.loc[\n",
    "        ((sorted_table['tpep_pickup_datetime'] >= left) &\n",
    "         (sorted_table['tpep_pickup_datetime'] < right)) |\n",
    "        ((sorted_table['tpep_dropoff_datetime'] >= left) &\n",
    "         (sorted_table['tpep_dropoff_datetime'] < right))]\n",
    "\n",
    "    # temp table to generate F,P,N layers\n",
    "    # keep snap intact\n",
    "    temp_snap = snap.loc[:, ['tripid',\n",
    "                             'tpep_pickup_datetime',\n",
    "                             'tpep_dropoff_datetime',\n",
    "                             'pulocationid',\n",
    "                             'dolocationid']]\n",
    "\n",
    "    # Use the interval to 'catch' corresponding trips.\n",
    "    # future layer\n",
    "    f_layer = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] < right) &\n",
    "                             (temp_snap['tpep_pickup_datetime'] >= left) &\n",
    "                             (temp_snap['tpep_dropoff_datetime'] >= right)]\n",
    "    # past layer\n",
    "    p_layer = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] < left) &\n",
    "                             (temp_snap['tpep_dropoff_datetime'] >= left) &\n",
    "                             (temp_snap['tpep_dropoff_datetime'] < right)]\n",
    "    # now layer\n",
    "    n_layer = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] >= left) &\n",
    "                             (temp_snap['tpep_dropoff_datetime'] < right)]\n",
    "\n",
    "    # Their count should add up to total trips caught\n",
    "    assert temp_snap.shape[0] == f_layer.shape[0] + p_layer.shape[0] + n_layer.shape[0]\n",
    "\n",
    "    return p_layer, n_layer, f_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the trips add up:\n",
    "print(snap.shape)\n",
    "print(f_flayer.shape[0] + f_player.shape[0] + f_nlayer.shape[0])\n",
    "assert snap.shape[0] == f_flayer.shape[0] + f_player.shape[0] + f_nlayer.shape[0]\n",
    "f_flayer.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pair of (pulocationid - 1, dolocationid - 1) indicates adding one to the snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generating Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before generating image, we have to map the zone id to a range from 0 to 54:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very important globals:\n",
    "real_id = list(map(str, list(y_zone.loc[:,'LocationID'])))\n",
    "conv_id = [i for i in range(img_size)]\n",
    "assert len(real_id) == len(conv_id)\n",
    "mp = dict(zip(real_id, conv_id))\n",
    "# the line below is a dirty fix, be causious in future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a snapshot:\n",
    "first_snapshot = np.zeros([img_size, img_size, 3], dtype='float64')\n",
    "print(first_snapshot.shape)\n",
    "print(first_snapshot[1,2,1])\n",
    "\n",
    "left_zones = set()\n",
    "\n",
    "# future-Red: 0  \n",
    "for _, row in f_flayer.iterrows():\n",
    "    try:\n",
    "        first_snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 0] += 1 # inplace increment 1 in numpy\n",
    "    except Exception as e:\n",
    "        left_zones.add(str(row['pulocationid']))\n",
    "        left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "# past-Green: 1\n",
    "for _, row in f_player.iterrows():\n",
    "    try:\n",
    "        first_snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 1] += 1\n",
    "    except Exception as e:\n",
    "        left_zones.add(str(row['pulocationid']))\n",
    "        left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "# now-Blue: 2        \n",
    "for _, row in f_nlayer.iterrows():\n",
    "    try:\n",
    "        first_snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 2] += 1\n",
    "    except Exception as e:\n",
    "        left_zones.add(str(row['pulocationid']))\n",
    "        left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "print(f'left_zones: {left_zones}')\n",
    "print(f'left_zones length: {len(left_zones)}')\n",
    "\n",
    "print(f'F max -> {first_snapshot[:,:,0].max()}')\n",
    "print(f'P max -> {first_snapshot[:,:,1].max()}')\n",
    "print(f'N max -> {first_snapshot[:,:,2].max()}')\n",
    "\n",
    "# convert numpy array to a image:\n",
    "# tb = pd.DataFrame(first_snapshot[:,:,0])\n",
    "first_snapshot *= 255//first_snapshot.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot FPN layer values:\n",
    "temp_f = np.reshape(first_snapshot[:,:,0], (1,-1))\n",
    "temp_p = np.reshape(first_snapshot[:,:,1], (1,-1))\n",
    "temp_n = np.reshape(first_snapshot[:,:,2], (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(temp_f, axlabel='Future (Red)', color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(temp_p, axlabel='Past (Green)', color='green');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(temp_n, axlabel='Now (Blue)', color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_snapshot = first_snapshot.astype('uint8')\n",
    "first_image = Image.fromarray(first_snapshot)\n",
    "first_image.resize((690,690))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image save directory:\n",
    "img_path = r'D:\\GITHUB\\GSPNet\\snapshots'\n",
    "\n",
    "# save image\n",
    "# first_image.save(f'{img_path}\\\\1.jpg')\n",
    "# first_image.resize((690,690)) # multiply by factor of 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put above codes into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that combines layers to an image\n",
    "def gen_image(p_layer, n_layer, f_layer):\n",
    "    '''\n",
    "    Generate an image using given matrices.\n",
    "    Params:\n",
    "        p_layer: matrix of past layer\n",
    "        n_layer: matrix of now layer\n",
    "        f_layer: matrix of future layer\n",
    "    Return:\n",
    "        A PIL image.\n",
    "    '''\n",
    "    # create a snapshot\n",
    "    snapshot = np.zeros([img_size, img_size, 3], dtype='float64')\n",
    "\n",
    "    # unexpected zones\n",
    "    left_zones = set()\n",
    "\n",
    "    # future-Red: 0\n",
    "    for _, row in f_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 0] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # past-Green: 1\n",
    "    for _, row in p_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 1] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # now-Blue: 2\n",
    "    for _, row in n_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 2] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # normalize\n",
    "    snapshot *= 255 // snapshot.max()\n",
    "    snapshot = snapshot.astype('uint8')\n",
    "    image = Image.fromarray(snapshot)\n",
    "    return image\n",
    "\n",
    "# generate tensor, because saving image as intermediate data is not efficient\n",
    "def gen_tensor(p_layer, n_layer, f_layer):\n",
    "    '''\n",
    "    Generate a tensor using given matrices.\n",
    "    Params:\n",
    "        p_layer: matrix of past layer\n",
    "        n_layer: matrix of now layer\n",
    "        f_layer: matrix of future layer\n",
    "    Return:\n",
    "        A torch tensor.\n",
    "    '''\n",
    "    # create a snapshot\n",
    "    snapshot = np.zeros([img_size, img_size, 3], dtype='float64')\n",
    "\n",
    "    # unexpected zones\n",
    "    left_zones = set()\n",
    "\n",
    "    # future-Red: 0\n",
    "    for _, row in f_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 0] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # past-Green: 1\n",
    "    for _, row in p_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 1] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # now-Blue: 2\n",
    "    for _, row in n_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 2] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # normalize\n",
    "    snapshot *= 255 // snapshot.max()\n",
    "    snapshot = torch.from_numpy(snapshot)\n",
    "    return snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets a specific layer of snapshot.\n",
    "def get_channel(image, layer:str):\n",
    "    '''\n",
    "    Get a layer of the snapshot.\n",
    "    Params:\n",
    "        image: PIL image\n",
    "        channel: one of R-F,G-P,B-N\n",
    "    Return:\n",
    "        single channel image\n",
    "    '''\n",
    "    assert layer in ['P', 'N', 'F']\n",
    "    namedict = {'P': 'G', 'N': 'B', 'F': 'R'}\n",
    "    chandict = {'R':0, 'G':1, 'B':2}\n",
    "    template = np.array(image)\n",
    "    chan = np.zeros([*template.shape], dtype='uint8')\n",
    "    chan[:,:,chandict[namedict[layer]]] = image.getchannel(namedict[layer])\n",
    "    chan = Image.fromarray(chan)\n",
    "    return chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future layer\n",
    "# first_image.getchannel('R').resize((345,345))\n",
    "red = get_channel(first_image, 'F')\n",
    "red = red.resize((345,345))\n",
    "red_bright = first_image.getchannel('R').resize((345,345))\n",
    "\n",
    "display(red, red_bright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Past layer\n",
    "green = get_channel(first_image, 'P')\n",
    "green = green.resize((345,345))\n",
    "\n",
    "# Past layer (green) in brightness.\n",
    "green_bright = first_image.getchannel('G').resize((345,345))\n",
    "display(green, green_bright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now layer\n",
    "# first_image.getchannel('B').resize((345,345))\n",
    "blue = get_channel(first_image, 'N')\n",
    "blue = blue.resize((345,345))\n",
    "\n",
    "# Now layer (blue) in brightness.\n",
    "blue_bright = first_image.getchannel('B').resize((345,345))\n",
    "\n",
    "display(blue, blue_bright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the above procedure, all 4464 tensors can be generated.\n",
    "\n",
    "**Possible variations:**\n",
    "+ Weight the value with passenger number\n",
    "+ Design a method to achieve finer granularity to distinguish trips with much longer time (3X std.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Parallel Batch Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a batch processing function to generate tonsors like above according to time interval. For the sake of effitiency, parallel boost is used (i.e. dask). The **computation graphs** of the generator is as follows:\n",
    "\n",
    "<img src=\"images/flowgraph_imggen.png\" align=\"left\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Individual functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of time bounds given the starting point and ending point of a time interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesplit(stp: str, etp: str, freq='10min'):\n",
    "    '''\n",
    "    Create a DatetimeIndx interval.\n",
    "    \n",
    "    Params:\n",
    "        stp: string, starting time point, first left bound\n",
    "        etp: string, ending time point, last right bound\n",
    "        freq: frequency, time interval unit of the splice operation\n",
    "    The stp and etp must of pattern \"yyyy-mm-dd hh:mm:ss\", otherwise exception will be raised.\n",
    "    \n",
    "    Return:\n",
    "        A list of time intervals tuples,each item is a tuple of two\n",
    "        interval(i.e., pandas.core.indexes.datetimes.DatetimeIndex object)\n",
    "        For example, a possible return could be [(2017-01-01 00:00:00, 2017-01-01 00:10:00),\n",
    "                                                 (2017-01-01 00:10:00, 2017-01-01 00:20:00)]\n",
    "    '''\n",
    "    pattern = re.compile('^([0-9]{4})-([0-1][0-9])-([0-3][0-9])\\s([0-1][0-9]|[2][0-3]):([0-5][0-9]):([0-5][0-9])$')\n",
    "    if pattern.match(stp) and pattern.match(etp):\n",
    "        time_bounds = pd.date_range(stp, etp, freq=freq)\n",
    "        sub_intervals = list(zip(time_bounds[:-1], time_bounds[1:]))\n",
    "        print(len(time_bounds), len(sub_intervals))\n",
    "        return sub_intervals\n",
    "    else:\n",
    "        raise Exception('Provided time bound is of invalid format.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timelist = timesplit('2017-01-01 00:00:00', '2017-02-01 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of characters needed to save a time pair\n",
    "sum(list(map(len, ['2017-01-01 00:00:00', '2017-02-01 00:00:00'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test functions\n",
    "timelist = timesplit('2017-01-01 00:00:00', '2017-02-01 00:00:00')\n",
    "print(f'timelist length: {len(timelist)}')\n",
    "\n",
    "bound = timelist[100]\n",
    "print(type(bound))\n",
    "print(bound[0])\n",
    "print(bound[1])\n",
    "\n",
    "f_player, f_nlayer, f_flayer = gen_snap_layers(table, bound)\n",
    "image = gen_image(f_player, f_nlayer, f_flayer)\n",
    "\n",
    "# image save directory:\n",
    "visual_path = r'D:\\GITHUB\\GSPNet\\snapshots\\visualization'\n",
    "data_path = r'D:\\GITHUB\\GSPNet\\snapshots\\data'\n",
    "\n",
    "# save image\n",
    "image.save(f'{data_path}\\\\d1.bmp')\n",
    "vimage = image.resize((690,690)) # multiply by factor of 100\n",
    "vimage.save(f'{visual_path}\\\\v1.bmp')\n",
    "vimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdtbt = time.time()\n",
    "timelist = timesplit('2017-02-01 00:00:00', '2017-03-01 00:00:00')\n",
    "table = pd.read_csv('dataset/nytaxi_yellow_2017_feb.csv')\n",
    "print(f'data loaded in {time.time() - rdtbt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# convert dtype for entire table here:\n",
    "table['tpep_pickup_datetime'] = pd.to_datetime(table['tpep_pickup_datetime'])\n",
    "table['tpep_dropoff_datetime'] = pd.to_datetime(table['tpep_dropoff_datetime'])\n",
    "for i, bound in enumerate(timelist):\n",
    "    p_layer, n_layer, f_layer = gen_snap_layers(table, bound)\n",
    "    tensor = gen_tensor(p_layer, n_layer, f_layer)\n",
    "    torch.save(tensor, f'{data_path}\\\\nytaxi_yellow_2017_feb_d{i}.pt')\n",
    "    image = gen_image(p_layer, n_layer, f_layer)\n",
    "    vimage = image.resize((690,690)) # multiply by factor of 100\n",
    "    vimage.save(f'{visual_path}\\\\nytaxi_yellow_2017_feb_v{i}.jpg')\n",
    "print(f'Image and tensor generation done in {time.time() - start :.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "26155 / 60 / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prediction & per demand modification of states (to be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
