{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition test\n",
    "\n",
    "In this notebook we try to train a model to predict traffic state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:41:57.964825Z",
     "start_time": "2019-04-07T12:41:56.632243Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob, iglob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:41:58.657641Z",
     "start_time": "2019-04-07T12:41:58.651657Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate training pair from tensors\n",
    "def batch_data(states, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "\n",
    "    Args:\n",
    "        states:\n",
    "        sequence_length: The sequence length of each batch\n",
    "        batch_size: The size of each batch; the number of sequences in a batch\n",
    "\n",
    "    Return:\n",
    "        DataLoader with batched data\n",
    "    \"\"\"\n",
    "    num_batches = len(states) // batch_size\n",
    "\n",
    "    # only full batches\n",
    "    states = states[: num_batches * batch_size]\n",
    "\n",
    "    # TODO: Implement function\n",
    "    features, targets = [], []\n",
    "\n",
    "    for idx in range(0, (len(states) - sequence_length)):\n",
    "        features.append(states[idx: idx + sequence_length])\n",
    "        targets.append(states[idx + sequence_length])\n",
    "\n",
    "    data = TensorDataset(torch.from_numpy(np.array(features)),\n",
    "                         torch.from_numpy(np.array(targets)))\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(data, shuffle=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # return a dataloader\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:41:59.651300Z",
     "start_time": "2019-04-07T12:41:59.648307Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dir = 'tensor_dataset/nn_test_15min/tensors'\n",
    "valid_dir = 'tensor_dataset/nn_test_15min_val/tensors'\n",
    "\n",
    "train_iter = iglob(train_dir + '/*')\n",
    "valid_iter = iglob(valid_dir + '/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:42:34.538478Z",
     "start_time": "2019-04-07T12:42:00.442249Z"
    }
   },
   "outputs": [],
   "source": [
    "train_states = []\n",
    "valid_states = []\n",
    "\n",
    "for state in train_iter:\n",
    "    state = torch.load(state).numpy()\n",
    "    train_states.append(state)\n",
    "    \n",
    "for state in valid_iter:\n",
    "    state = torch.load(state).numpy()\n",
    "    valid_states.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:42:59.530672Z",
     "start_time": "2019-04-07T12:42:59.525713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5856, 2976)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_states), len(valid_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:43:28.376517Z",
     "start_time": "2019-04-07T12:43:27.791107Z"
    }
   },
   "outputs": [],
   "source": [
    "train_states = np.array(train_states)\n",
    "valid_states = np.array(valid_states)\n",
    "\n",
    "train_states = train_states.reshape((len(train_states), -1))\n",
    "valid_states = valid_states.reshape((len(valid_states), -1))\n",
    "train_states = train_states.astype('float32')\n",
    "valid_states = valid_states.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:43:42.914932Z",
     "start_time": "2019-04-07T12:43:39.032318Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = batch_data(train_states, 12, 2)\n",
    "valid_loader = batch_data(valid_states, 12, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T01:29:51.650340Z",
     "start_time": "2019-04-07T01:29:51.640367Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_iter = iter(data_loader)\n",
    "# sample_x, sample_y = data_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T01:29:51.654330Z",
     "start_time": "2019-04-07T01:29:51.651337Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample_x.shape, sample_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T01:29:51.661311Z",
     "start_time": "2019-04-07T01:29:51.655327Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample_y[0][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:43:48.029422Z",
     "start_time": "2019-04-07T12:43:48.001496Z"
    }
   },
   "outputs": [],
   "source": [
    "class StateRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_dim=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        \n",
    "        # define the LSTM\n",
    "        self.lstm = nn.LSTM(input_size, self.hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        # define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, self.output_size)\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:43:49.942131Z",
     "start_time": "2019-04-07T12:43:49.935126Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden, clip):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"    \n",
    "    # TODO: Implement Function\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in hidden])\n",
    "        \n",
    "    # zero accumulated gradients\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    # print(f'input shape: {inp}, target shape: {target}')\n",
    "    # get the output from the model\n",
    "    output, h = rnn(inp, h)\n",
    "\n",
    "    # perform backpropagation and optimization\n",
    "    # calculate the loss and perform backprop\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    # 'clip_grad_norm' helps prevent the exploding gradient problem in RNNs / LSTMs\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T01:29:51.684249Z",
     "start_time": "2019-04-07T01:29:51.678265Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, clip=5, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset) // batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            # print(f'inputs shape: {inputs.shape} labels shape: {labels.shape}')\n",
    "            # print(f'inputs dtype: {inputs[0][0][0].dtype} label shape: {labels[0][0].dtype}')\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden, clip)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "            if (batch_i % show_every_n_batches == 0):\n",
    "                # printing loss stats\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:44:04.615865Z",
     "start_time": "2019-04-07T12:44:04.605892Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_rnn(model, batch_size, optimizer, criterion, n_epochs,\n",
    "              train_loader, valid_loader, clip=5, show_every_n_batches=100):\n",
    "    '''\n",
    "    Train a RNN model with the given hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        model:\n",
    "        batch_size:\n",
    "        optimizer:\n",
    "        criterion:\n",
    "        n_epochs:\n",
    "        train_loader:\n",
    "        valid_loader:\n",
    "        clip:\n",
    "        show_every_batches:\n",
    "\n",
    "    Returns:\n",
    "        A trained model\n",
    "    '''\n",
    "    batch_losses = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "\n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "\n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset) // batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "\n",
    "            # forward, back prop\n",
    "            # print(f'inputs shape: {inputs.shape} labels shape: {labels.shape}')\n",
    "            # print(f'inputs dtype: {inputs[0][0][0].dtype} label shape: {labels[0][0].dtype}')\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            loss, hidden = forward_back_prop(\n",
    "                model, optimizer, criterion, inputs, labels, hidden, clip\n",
    "            )\n",
    "\n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "            \n",
    "            # print loss every show_every_n_batches batches\n",
    "            # including validation loss\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                # get validation loss\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "\n",
    "                # switch to validation mode\n",
    "                model.eval()\n",
    "\n",
    "                for v_inputs, v_labels in valid_loader:\n",
    "\n",
    "                    v_inputs, v_labels = v_inputs.cuda(), v_labels.cuda()\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    v_output, val_h = model(v_inputs, val_h)\n",
    "                    val_loss = criterion(v_output, v_labels)\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                # printing loss stats\n",
    "                print(f'Epoch: {epoch_i:>4}/{n_epochs:<4}  Loss: {np.average(batch_losses)}  Val Loss {np.mean(val_losses)}')\n",
    "                batch_losses = []\n",
    "                val_losses = []\n",
    "\n",
    "    # returns a trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:44:05.771632Z",
     "start_time": "2019-04-07T12:44:05.767666Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(filename, decoder):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    torch.save(decoder, save_filename)\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n",
    "    return torch.load(save_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:44:20.621320Z",
     "start_time": "2019-04-07T12:44:20.617331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 12 # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 2\n",
    "# Gradient clip\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T12:44:22.424488Z",
     "start_time": "2019-04-07T12:44:22.420499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 20\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Output size\n",
    "output_size = 69*69*3\n",
    "\n",
    "# Hidden Dimension\n",
    "hidden_dim = 512\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "senb = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-07T12:44:33.205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateRNN(\n",
      "  (lstm): LSTM(14283, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=14283, bias=True)\n",
      ")\n",
      "Training for 20 epoch(s)...\n",
      "Epoch:    1/20    Loss: 0.5704490595720708\n",
      " Val Loss 0.46351306287473754\n",
      "Epoch:    1/20    Loss: 0.5361692968700081\n",
      " Val Loss 0.5316795544816978\n",
      "Epoch:    2/20    Loss: 0.47732206137073646\n",
      " Val Loss 0.40610086521593786\n",
      "Epoch:    2/20    Loss: 0.48377808824181556\n",
      " Val Loss 0.41752248115375057\n",
      "Epoch:    3/20    Loss: 0.4276056278760581\n",
      " Val Loss 0.40269187472222745\n",
      "Epoch:    3/20    Loss: 0.45605226010642946\n",
      " Val Loss 0.422963288895454\n",
      "Epoch:    4/20    Loss: 0.42413972480693574\n",
      " Val Loss 0.38509289833104965\n",
      "Epoch:    4/20    Loss: 0.44726190896891055\n",
      " Val Loss 0.40608991398356664\n",
      "Epoch:    5/20    Loss: 0.41287255661540767\n",
      " Val Loss 0.3924369770824889\n",
      "Epoch:    5/20    Loss: 0.4387126291282475\n",
      " Val Loss 0.3838955447726283\n",
      "Epoch:    6/20    Loss: 0.40602778038930665\n",
      " Val Loss 0.39617976384908565\n",
      "Epoch:    6/20    Loss: 0.4329397416114807\n",
      " Val Loss 0.3940083388146139\n",
      "Epoch:    7/20    Loss: 0.40340273690184153\n",
      " Val Loss 0.39271165643455447\n",
      "Epoch:    7/20    Loss: 0.43283079205639663\n",
      " Val Loss 0.4063268726208975\n",
      "Epoch:    8/20    Loss: 0.3983608738458197\n",
      " Val Loss 0.38285905976155105\n",
      "Epoch:    8/20    Loss: 0.4266490147076547\n",
      " Val Loss 0.3957555026265914\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = True\n",
    "\n",
    "# create model and move to gpu if available\n",
    "# def __init__(self, input_size, output_size, hidden_dim=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "rnn = StateRNN(14283, 14283)\n",
    "rnn = rnn.cuda()\n",
    "    \n",
    "# print the details of model\n",
    "print(rnn)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, train_loader, valid_loader, show_every_n_batches=senb)\n",
    "\n",
    "# saving the trained model\n",
    "save_model(f'./trained_models/sl{sequence_length}-bs{batch_size}-lr{learning_rate}-hd{hidden_dim}-nl{n_layers}', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
