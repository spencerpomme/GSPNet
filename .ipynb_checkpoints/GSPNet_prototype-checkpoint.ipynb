{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo_city.png\" align=\"right\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Graph State Prediction Network (GSPNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a prototype on how to train a graph state prediction network using CNN and LSTM.\n",
    "The content is devided into __3__ parts:\n",
    "\n",
    "1. Data preprocessing\n",
    "2. Model building, Training and Tuning\n",
    "3. Prediction and per demand modifacation\n",
    "\n",
    "The model is built with [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use pandas and numpy as main data examine and cleansing util and use dask to handle parallel computing cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlalchemy\n",
    "import dask\n",
    "import time\n",
    "import psycopg2\n",
    "import warnings\n",
    "import re\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first, use a small sample data (approximately 1 million rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already cleaned data sample\n",
    "table = pd.read_csv('dataset/nytaxi_yellow_2017_jan.csv')\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. First sight of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the the type of columns and some basic statistics of the sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column data types\n",
    "table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows and columns\n",
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic statistics\n",
    "table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we will use are: **tpep_pickup_datetime, pulocationid, tpep_dropoff_datetime, dolocationid.**\n",
    "These information are used to generate graphs that are used to discribe the general traffic state of New York.\n",
    "\n",
    "Notice currently the **tpep_pickup_datetime** and **tpep_dropoff_datetime** are of type `object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.loc[:,['tpep_pickup_datetime', 'tpep_dropoff_datetime']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do **time slice** later, these two feilds must be sorted in chronological mananer, first by `tpep_pickup_datetime` and then by `tpep_dropoff_datetime`, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # pick first 10 rows\n",
    "table.sort_values(by=['tpep_pickup_datetime', 'tpep_dropoff_datetime']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that the taxi trips are nicely sorted according to **time**. What we need to do is convert this tabular date into graphs that described by **adjacency matrice**.\n",
    "\n",
    "First, let's see what are `pulocationid` and `dolocationid`:\n",
    "\n",
    "<img src=\"images/map.jpg\" align=\"center\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are code number for a district in Manhattan, New York. We will mainly describe the traffic state and prediction at **this** level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Time slicing\n",
    "\n",
    "Now we've done a very superficial exploration of the data. It's time to convert the tabular data to graphs that are to be fed into our neural network model.\n",
    "\n",
    "The first problem is, **how to decide a feasible time interval?**\n",
    "\n",
    "Of course, the time interval should be a varaible that is to be selected **per need. However,** it is a reasonable thinking to let a time interval (i.e. a **snapshot**) to contain as many as **complete trip** as possible. See the diagram below:\n",
    "\n",
    "<img src=\"images/snapshot.png\" align=\"center\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **If the time interval is too small**:\n",
    "\n",
    "The majority of the matrices will be very sparse or even completely blank.\n",
    "\n",
    "#### If the time interval is too big:\n",
    "\n",
    "Then information about trips will be densely squeezed into one matrix and a lot of details on changes along time will be lost.\n",
    "\n",
    "Thus, it is important to choose time interval wisely. Let's look at the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.loc[:,['trip_distance', 'trip_time_sec', 'trip_avg_speed']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The an average trip lasts for roughly 900 seconds, which is **15 minutes**.\n",
    "\n",
    "The median(i.e. 50% percentile) is roughly 600 seconds, which is **10 minutes**.\n",
    "\n",
    "Let's plot the distribution of column `trip_time_sec` to have a better intuition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "sns.set(color_codes=True)\n",
    "sns.distplot(table.loc[:,'trip_time_sec']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the trips are mostly concentrated in a shorter time range, while some outliers may significantly biased the average.\n",
    "\n",
    "Let's see how how many trips are longer than 30 minutes (1800 secs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows with column 'trip_time_sec' value larger than 1800 divided by total number of rows\n",
    "table.loc[table['trip_time_sec'] > 1800].shape[0] / 1040001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **95%** of the trips last shorter than 30 minutes, let's plot distribution at this range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distribution of 95% trip data in time duration\n",
    "temp = table.loc[table['trip_time_sec'] <= 1800]\n",
    "sns.distplot(temp.loc[:,'trip_time_sec']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# statistics on the 95% data\n",
    "temp.loc[:,['trip_distance', 'trip_time_sec', 'trip_avg_speed']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value and their appearance\n",
    "pd.DataFrame(temp.loc[:,'trip_time_sec'].value_counts().sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 50% of the trips have trip time between 6 to 15 minutes, with a median of 10 minutes.\n",
    "\n",
    "#### As a result, we primarily decide to use 10 minutes as time interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. From tabular data to graph\n",
    "\n",
    "Since we have decided a time interval, it's time to generate matrices. Basically, the idea is shown as figure below:\n",
    "\n",
    "<img src=\"images/matrices.png\" align=\"left\" width=\"85%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from **outgoin** and **incoming** matrices shown above, an additional **Domestic** matrix is add because 50% of trips last less than the **chosen interval 10 minutes**. Such matrices can be combined into a **Tensor** illustrated as follows:\n",
    "\n",
    "<img src=\"images/layers.png\" align=\"center\" width=\"35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STRICT DEFINITION OF OID layers\n",
    "\n",
    "+ **O**utgoing layer\n",
    "    Condition: \n",
    "    1. subtable sorted by tpep_**pickup**_datetime\n",
    "    2. left bound of interval  <= tpep_pickup_datetime  < right bound of interval\n",
    "    3. right bound of interval <= tpep_dropoff_datetime\n",
    "\n",
    "\n",
    "+ **I**ncoming layer\n",
    "    Condition: \n",
    "    1. subtable sorted by tpep_**dropoff**_datetime\n",
    "    2. left bound of interval  <= tpep_dropoff_datetime  < right bound of interval\n",
    "    3. tpep_pickup_datetime < left bound of interval\n",
    "\n",
    "\n",
    "+ **D**omestic layer\n",
    "    Condition: \n",
    "    1. subtable sorted by either tpep\\_**pickup**\\_datetime or tpep\\_**dropoff**\\_datetime, but only count **once*.\n",
    "    2. left bound of interval  <= tpep_pickup_datetime < tpep_dropoff_datetime  < right bound of interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract relevant columns:\n",
    "\n",
    "To generate tensor defined above, **four** key columns are needed.They are `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `pulocationid` and `dolocationid`.\n",
    "\n",
    "We first extract them from original table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw four needed columns and a id column, then sort according to time.\n",
    "# the `tripid` column is for the sake of naming.\n",
    "tensor_gen_o = table.loc[:,['tripid',\n",
    "                          'tpep_pickup_datetime',\n",
    "                          'tpep_dropoff_datetime',\n",
    "                          'pulocationid', 'dolocationid']\n",
    "                      ].sort_values(by=['tpep_pickup_datetime', 'tpep_dropoff_datetime']) # the first sort condition rules\n",
    "tensor_gen_o.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice the table to sub-tables according to time interval (10 min)\n",
    "\n",
    "Since the columns are extracted, now it's time to generate tensors.\n",
    "\n",
    "The dataset is trip data of all yellow taxi cabs in Manhattan area in **January**, 2017.As we are making 10-minute-splices, there will be total:\n",
    "\n",
    "$$\n",
    "6 \\times 24 \\times 31 = 4464 \\space slices(i.e. \\space snapshots)\n",
    "$$\n",
    "\n",
    "Each slice has 3 layers, that is in total\n",
    "\n",
    "$$\n",
    "4464 \\times 3 = 13392 \\space matrices\n",
    "$$\n",
    "to be generated. \n",
    "\n",
    "First, let's try to generate one image (i.e. a *tensor* with **3** layers).\n",
    "\n",
    "The time columns need to be converted to pandas timestamp type in order to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current type is python str\n",
    "type(tensor_gen_o.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to timestamp\n",
    "pd.to_datetime(tensor_gen_o.iloc[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.to_datetime(tensor_gen_o.iloc[0,1]) # 2017-01-01 00:00:00\n",
    "t2 = pd.to_datetime(tensor_gen_o.iloc[0,2]) # 2017-01-01 00:01:00\n",
    "t1 < t2 # test if can compare time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply type cast to all values:\n",
    "tensor_gen_o['tpep_pickup_datetime'] = pd.to_datetime(tensor_gen_o['tpep_pickup_datetime'])\n",
    "tensor_gen_o['tpep_dropoff_datetime'] = pd.to_datetime(tensor_gen_o['tpep_dropoff_datetime'])\n",
    "\n",
    "tensor_gen_o.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_gen_o.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interval **boundaries** are fixed when interval is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create intervals\n",
    "intervals = pd.date_range('2017-01-01 00:00:00', '2017-02-01 00:00:00', freq='10min')\n",
    "\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: When splicing time interval int such manner, we are assuming the **entire** month is monitored. Under this setting, we do not consider another day is a 'fresh start'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First subtb:\n",
    "first = tensor_gen_o.loc[tensor_gen_o['tpep_pickup_datetime'] < intervals[1]]\n",
    "print(f'shape is: {first.shape}')\n",
    "first.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort to have a clearer look:\n",
    "first.sort_values(by=['pulocationid', 'dolocationid']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `first` will generate Outgoing, Incoming and Domestic layers, that is, 3 matrices of size: \n",
    "\n",
    "$$\n",
    "(number \\space of\\space zones)\\space \\times \\space (number\\space of\\space zones)\n",
    "$$\n",
    "\n",
    "Let's see the zone numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load zone lookup table\n",
    "zones = pd.read_csv('dataset/taxi_zone_lookup.csv')\n",
    "print(f'shape is: {zones.shape}')\n",
    "zones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 265 zones, indexing from 0 to 264.\n",
    "\n",
    "Among which, we only care about `Yellow Zone` or `Manhattan` area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_zone = zones.loc[zones['Borough'] == 'Manhattan']\n",
    "print(y_zone.shape)\n",
    "img_size = y_zone.shape[0]\n",
    "y_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the layers should be matrices of size (69, 69), and each timeslice(snapshot) should be tensors of size (69, 69, 3).\n",
    "\n",
    "Create the first tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dtype for entire table here:\n",
    "table['tpep_pickup_datetime'] = pd.to_datetime(table['tpep_pickup_datetime'])\n",
    "table['tpep_dropoff_datetime'] = pd.to_datetime(table['tpep_dropoff_datetime'])\n",
    "\n",
    "# O,I,D layers of first snapshot defined:\n",
    "tensor_gen_o = table.loc[:,['tripid',\n",
    "                          'tpep_pickup_datetime',\n",
    "                          'tpep_dropoff_datetime',\n",
    "                          'pulocationid', 'dolocationid']\n",
    "                      ].sort_values(by=['tpep_pickup_datetime', 'tpep_dropoff_datetime']) # the first sort condition rules\n",
    "\n",
    "tensor_gen_I = table.loc[:,['tripid',\n",
    "                          'tpep_pickup_datetime',\n",
    "                          'tpep_dropoff_datetime',\n",
    "                          'pulocationid', 'dolocationid']\n",
    "                      ].sort_values(by=['tpep_dropoff_datetime', 'tpep_pickup_datetime']) # the first sort condition rules\n",
    "\n",
    "tensor_gen_D = table.loc[:,['tripid',\n",
    "                          'tpep_pickup_datetime',\n",
    "                          'tpep_dropoff_datetime',\n",
    "                          'pulocationid', 'dolocationid']\n",
    "                      ].sort_values(by=['tpep_dropoff_datetime', 'tpep_pickup_datetime']) # whichever is ok\n",
    "\n",
    "# Their shape should be the same\n",
    "assert tensor_gen_o.shape == tensor_gen_I.shape == tensor_gen_D.shape\n",
    "\n",
    "# Check the condition of three layers above, if forgotten.\n",
    "f_olayer = tensor_gen_o.loc[(tensor_gen_o['tpep_pickup_datetime'] < intervals[1]) &\n",
    "                            (tensor_gen_o['tpep_pickup_datetime'] >= intervals[0]) &\n",
    "                            (tensor_gen_o['tpep_dropoff_datetime'] >= intervals[1])\n",
    "                           ]\n",
    "\n",
    "f_Ilayer = tensor_gen_I.loc[(tensor_gen_I['tpep_pickup_datetime'] < intervals[0]) &\n",
    "                            (tensor_gen_I['tpep_dropoff_datetime'] >= intervals[0]) &\n",
    "                            (tensor_gen_I['tpep_dropoff_datetime'] < intervals[1])\n",
    "                           ]\n",
    "\n",
    "f_Dlayer = tensor_gen_D.loc[(tensor_gen_D['tpep_pickup_datetime'] >= intervals[0]) &\n",
    "                            (tensor_gen_D['tpep_dropoff_datetime'] < intervals[1])\n",
    "                           ]\n",
    "\n",
    "print(f'f_olayer.shape: {f_olayer.shape}')\n",
    "print(f'f_Ilayer.shape: {f_Ilayer.shape}') # There will be no incoming trips for the first snapshot.\n",
    "print(f'f_Dlayer.shape: {f_Dlayer.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the trips add up:\n",
    "assert first.shape[0] == f_olayer.shape[0] + f_Ilayer.shape[0] + f_Dlayer.shape[0]\n",
    "f_olayer.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pair of (pulocationid - 1, dolocationid - 1) indicates adding one to the snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generating Image\n",
    "Before generating image, we have to map the zone id to a range from 0 to 54:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_id = list(map(str, list(y_zone.loc[:,'LocationID'])))\n",
    "conv_id = [i for i in range(img_size)]\n",
    "assert len(real_id) == len(conv_id)\n",
    "mp = dict(zip(real_id, conv_id))\n",
    "# the line below is a dirty fix, be causious in future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a snapshot:\n",
    "first_snapshot = np.zeros([img_size, img_size, 3], dtype='float64')\n",
    "print(first_snapshot.shape)\n",
    "print(first_snapshot[1,2,1])\n",
    "\n",
    "left_zones = set()\n",
    "\n",
    "for _, row in f_olayer.iterrows():\n",
    "    try:\n",
    "        first_snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 0] += 1 # inplace increment 1 in numpy\n",
    "    except Exception as e:\n",
    "        left_zones.add(str(row['pulocationid']))\n",
    "        left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "for _, row in f_Ilayer.iterrows():\n",
    "    try:\n",
    "        first_snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 1] += 1\n",
    "    except Exception as e:\n",
    "        left_zones.add(str(row['pulocationid']))\n",
    "        left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "for _, row in f_Dlayer.iterrows():\n",
    "    try:\n",
    "        first_snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 2] += 1\n",
    "    except Exception as e:\n",
    "        left_zones.add(str(row['pulocationid']))\n",
    "        left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "print(f'left_zones: {left_zones}')\n",
    "print(f'left_zones length: {len(left_zones)}')\n",
    "\n",
    "print(f'O max -> {first_snapshot[:,:,0].max()}')\n",
    "print(f'I max -> {first_snapshot[:,:,1].max()}')\n",
    "print(f'D max -> {first_snapshot[:,:,2].max()}')\n",
    "\n",
    "# convert numpy array to a image:\n",
    "tb = pd.DataFrame(first_snapshot[:,:,0])\n",
    "\n",
    "\n",
    "first_snapshot *= 255//first_snapshot.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot OID layer values:\n",
    "temp_o = np.reshape(first_snapshot[:,:,0], (1,-1))\n",
    "temp_i = np.reshape(first_snapshot[:,:,1], (1,-1))\n",
    "temp_d = np.reshape(first_snapshot[:,:,2], (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(temp_o, axlabel='out-going(Red)', color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(temp_i, axlabel='in-coming(Green)', color='green');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(temp_d, axlabel='domestic(Blue)', color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_snapshot = first_snapshot.astype('uint8')\n",
    "first_image = Image.fromarray(first_snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image.resize((690,690)) # multiply by factor of 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the above procedure, all 4464 tensors can be generated.\n",
    "\n",
    "**Possible variations:**\n",
    "+ Weight the value with passenger number\n",
    "+ Design a method to achieve finer granularity to distinguish trips with much longer time (3X std.)\n",
    "\n",
    "Now let's define a batch processing function to generate tonsors like above according to time interval. For the sake of effitiency, parallel boost is used (i.e. dask). The **computation graphs** of the generator is as follows:\n",
    "\n",
    "<img src=\"images/flowgraph_imggen.png\" align=\"left\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervals(stp: str, etp: str, freq='10min'):\n",
    "    '''\n",
    "    Create a DatetimeIndx interval.\n",
    "    Params:\n",
    "        stp: string, starting time point, first left bound\n",
    "        etp: string, ending time point, last right bound\n",
    "        freq: frequency, time interval unit of the splice operation\n",
    "    The stp and etp must of pattern \"yyyy-mm-dd hh:mm:ss\", otherwise exception will be raised.\n",
    "    Return:\n",
    "        A list of time intervals,(i.e., pandas.core.indexes.datetimes.DatetimeIndex object)\n",
    "    '''\n",
    "    \n",
    "    pd.date_range('2017-01-01 00:00:00', '2017-02-01 00:00:00', freq='10min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to generate such images in parallel\n",
    "class ImageGenerator:\n",
    "    '''\n",
    "    Generate image from tabular data with specified setting of time interval.\n",
    "    '''\n",
    "    def __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prediction & per demand modification of states (to be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
