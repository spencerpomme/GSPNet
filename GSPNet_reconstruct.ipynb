{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo_city.png\" align=\"right\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Graph State Prediction Network (GSPNet): Simplified version\n",
    "\n",
    "This notebook is a prototype on how to train a graph state prediction network using CNN and LSTM.\n",
    "The content is devided into __3__ parts:\n",
    "\n",
    "1. Data preprocessing\n",
    "2. Model building, Training and Tuning\n",
    "3. Prediction and per demand modifacation\n",
    "\n",
    "The model is built with [PyTorch](https://pytorch.org/).\n",
    "\n",
    "<p style='color: darkred'><strong>This version is simplified and most of intermediate explainatory codes and comments are removed except those directly contribute to generating images. For duck process, see the prototype version.</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "### 1. Load Libraries and Data\n",
    "\n",
    "Load in the data. Check the data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sqlalchemy\n",
    "import time\n",
    "import psycopg2\n",
    "import warnings\n",
    "import re\n",
    "import torch\n",
    "from dask import dataframe as dd\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for test only. Use dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripid</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>pulocationid</th>\n",
       "      <th>dolocationid</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>trip_avg_speed</th>\n",
       "      <th>trip_time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35972297</td>\n",
       "      <td>2017-03-09 21:30:11</td>\n",
       "      <td>2017-03-09 21:44:20</td>\n",
       "      <td>148</td>\n",
       "      <td>48</td>\n",
       "      <td>4.06</td>\n",
       "      <td>1</td>\n",
       "      <td>18.36</td>\n",
       "      <td>00:14:09</td>\n",
       "      <td>17.215548</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35972298</td>\n",
       "      <td>2017-03-09 21:47:00</td>\n",
       "      <td>2017-03-09 21:58:01</td>\n",
       "      <td>48</td>\n",
       "      <td>107</td>\n",
       "      <td>2.73</td>\n",
       "      <td>1</td>\n",
       "      <td>12.80</td>\n",
       "      <td>00:11:01</td>\n",
       "      <td>14.868381</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35972299</td>\n",
       "      <td>2017-03-09 22:01:08</td>\n",
       "      <td>2017-03-09 22:11:16</td>\n",
       "      <td>79</td>\n",
       "      <td>162</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1</td>\n",
       "      <td>14.12</td>\n",
       "      <td>00:10:08</td>\n",
       "      <td>13.440789</td>\n",
       "      <td>608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35972300</td>\n",
       "      <td>2017-03-09 22:16:05</td>\n",
       "      <td>2017-03-10 06:26:11</td>\n",
       "      <td>237</td>\n",
       "      <td>41</td>\n",
       "      <td>3.86</td>\n",
       "      <td>1</td>\n",
       "      <td>17.29</td>\n",
       "      <td>08:10:06</td>\n",
       "      <td>0.472557</td>\n",
       "      <td>29406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35972301</td>\n",
       "      <td>2017-03-31 06:31:53</td>\n",
       "      <td>2017-03-31 06:41:48</td>\n",
       "      <td>41</td>\n",
       "      <td>162</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1</td>\n",
       "      <td>13.30</td>\n",
       "      <td>00:09:55</td>\n",
       "      <td>20.873950</td>\n",
       "      <td>595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tripid tpep_pickup_datetime tpep_dropoff_datetime  pulocationid  \\\n",
       "0  35972297  2017-03-09 21:30:11   2017-03-09 21:44:20           148   \n",
       "1  35972298  2017-03-09 21:47:00   2017-03-09 21:58:01            48   \n",
       "2  35972299  2017-03-09 22:01:08   2017-03-09 22:11:16            79   \n",
       "3  35972300  2017-03-09 22:16:05   2017-03-10 06:26:11           237   \n",
       "4  35972301  2017-03-31 06:31:53   2017-03-31 06:41:48            41   \n",
       "\n",
       "   dolocationid  trip_distance  passenger_count  total_amount trip_time  \\\n",
       "0            48           4.06                1         18.36  00:14:09   \n",
       "1           107           2.73                1         12.80  00:11:01   \n",
       "2           162           2.27                1         14.12  00:10:08   \n",
       "3            41           3.86                1         17.29  08:10:06   \n",
       "4           162           3.45                1         13.30  00:09:55   \n",
       "\n",
       "   trip_avg_speed  trip_time_sec  \n",
       "0       17.215548            849  \n",
       "1       14.868381            661  \n",
       "2       13.440789            608  \n",
       "3        0.472557          29406  \n",
       "4       20.873950            595  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = dd.read_csv('dataset/nytaxi_yellow_2017_mar.csv')\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check table shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Delayed('int-127fd34d-9d1b-4be0-a077-e5e2592cab84'), 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data:\n",
    "\n",
    "Ratio of trips last longer than 30 minutes: (better smaller than 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Delayed('truediv-8e37a89276df7678b48caff104c65f77')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(table.loc[table['trip_time_sec'] > 1800].shape[0] / table.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Time Interval Preprocessing\n",
    "\n",
    "Create time interval and process original table to generate images.\n",
    "\n",
    "Change datetime columns from str type to Timestamp.\n",
    "\n",
    "Function to create time **intervals**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesplit(stp: str, etp: str, freq='10min'):\n",
    "    '''\n",
    "    Create a DatetimeIndx interval.\n",
    "    \n",
    "    Params:\n",
    "        stp: string, starting time point, first left bound\n",
    "        etp: string, ending time point, last right bound\n",
    "        freq: frequency, time interval unit of the splice operation\n",
    "    The stp and etp must of pattern \"yyyy-mm-dd hh:mm:ss\", otherwise exception will be raised.\n",
    "    \n",
    "    Return:\n",
    "        A list of time intervals tuples,each item is a tuple of two\n",
    "        interval(i.e., pandas.core.indexes.datetimes.DatetimeIndex object)\n",
    "        For example, a possible return could be [(2017-01-01 00:00:00, 2017-01-01 00:10:00),\n",
    "                                                 (2017-01-01 00:10:00, 2017-01-01 00:20:00)]\n",
    "    '''\n",
    "    pattern = re.compile('^([0-9]{4})-([0-1][0-9])-([0-3][0-9])\\s([0-1][0-9]|[2][0-3]):([0-5][0-9]):([0-5][0-9])$')\n",
    "    if pattern.match(stp) and pattern.match(etp):\n",
    "        time_bounds = pd.date_range(stp, etp, freq=freq)\n",
    "        sub_intervals = list(zip(time_bounds[:-1], time_bounds[1:]))\n",
    "        print(len(time_bounds), len(sub_intervals))\n",
    "        return sub_intervals\n",
    "    else:\n",
    "        raise Exception('Provided time bound is of invalid format.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Location Preprocessing\n",
    "\n",
    "Preprocess location related information: mapping ids and locations, then generate adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape is: (265, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load zone lookup table\n",
    "zones = pd.read_csv('dataset/taxi_zone_lookup.csv')\n",
    "print(f'shape is: {zones.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69, 4)\n"
     ]
    }
   ],
   "source": [
    "# very important globals:\n",
    "yellow_zone = zones.loc[zones['Borough'] == 'Manhattan']\n",
    "print(yellow_zone.shape)\n",
    "img_size = yellow_zone.shape[0]\n",
    "\n",
    "real_id = list(map(str, list(yellow_zone.loc[:,'LocationID'])))\n",
    "conv_id = [i for i in range(img_size)]\n",
    "assert len(real_id) == len(conv_id)\n",
    "mp = dict(zip(real_id, conv_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate Tensor: 3 matrices (layers) of connection\n",
    "\n",
    "Generate images that represent traffic states.\n",
    "\n",
    "Define functions to generate desired outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_snap_layers(table, bound):\n",
    "    '''\n",
    "    Generate Past layer, Now layer and Future layer for one snapshot.\n",
    "    Params:\n",
    "        table:\n",
    "        bounds:\n",
    "    Return:\n",
    "        PNF layers, a list.\n",
    "    '''\n",
    "    # left bound and right bound of time interval\n",
    "    assert type(bound) == tuple\n",
    "    left = bound[0]\n",
    "    right = bound[1]\n",
    "    \n",
    "    print(left, right)\n",
    "    \n",
    "    # no need to sort table indeed?\n",
    "    projected_table = table.loc[:, ['tripid',\n",
    "                                    'tpep_pickup_datetime',\n",
    "                                    'tpep_dropoff_datetime',\n",
    "                                    'pulocationid',\n",
    "                                    'dolocationid']]\n",
    "\n",
    "    # The condition of making snapshot should be:\n",
    "    # at least one temporal end of a trip should be within the bounds:\n",
    "    snap = projected_table.loc[\n",
    "        ((projected_table['tpep_pickup_datetime'] >= left) &\n",
    "         (projected_table['tpep_pickup_datetime'] < right)) |\n",
    "        ((projected_table['tpep_dropoff_datetime'] >= left) &\n",
    "         (projected_table['tpep_dropoff_datetime'] < right))]\n",
    "\n",
    "    # temp table to generate F,P,N layers\n",
    "    # keep snap intact\n",
    "    temp_snap = snap.copy()\n",
    "\n",
    "    # Use the interval to 'catch' corresponding trips.\n",
    "    # future layer\n",
    "    f_layer = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] < right) &\n",
    "                             (temp_snap['tpep_pickup_datetime'] >= left) &\n",
    "                             (temp_snap['tpep_dropoff_datetime'] >= right)]\n",
    "    # past layer\n",
    "    p_layer = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] < left) &\n",
    "                             (temp_snap['tpep_dropoff_datetime'] >= left) &\n",
    "                             (temp_snap['tpep_dropoff_datetime'] < right)]\n",
    "    # now layer\n",
    "    n_layer = temp_snap.loc[(temp_snap['tpep_pickup_datetime'] >= left) &\n",
    "                             (temp_snap['tpep_dropoff_datetime'] < right)]\n",
    "\n",
    "    # Their count should add up to total trips caught\n",
    "    assert temp_snap.shape[0] == f_layer.shape[0] + p_layer.shape[0] + n_layer.shape[0]\n",
    "\n",
    "    return p_layer, n_layer, f_layer\n",
    "\n",
    "\n",
    "# Function that combines layers to an image\n",
    "def gen_image(p_layer, n_layer, f_layer):\n",
    "    '''\n",
    "    Generate an image using given matrices.\n",
    "    Params:\n",
    "        p_layer: matrix of past layer\n",
    "        n_layer: matrix of now layer\n",
    "        f_layer: matrix of future layer\n",
    "    Return:\n",
    "        A PIL image.\n",
    "    '''\n",
    "    # create a snapshot\n",
    "    snapshot = np.zeros([img_size, img_size, 3], dtype='float64')\n",
    "\n",
    "    # unexpected zones\n",
    "    left_zones = set()\n",
    "\n",
    "    # future-Red: 0\n",
    "    for _, row in f_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 0] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # past-Green: 1\n",
    "    for _, row in p_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 1] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # now-Blue: 2\n",
    "    for _, row in n_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 2] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # normalize\n",
    "    snapshot *= 255 // snapshot.max()\n",
    "    snapshot = snapshot.astype('uint8')\n",
    "    image = Image.fromarray(snapshot)\n",
    "    return image\n",
    "\n",
    "\n",
    "# generate tensor, because saving image as intermediate data is not efficient\n",
    "def gen_tensor(p_layer, n_layer, f_layer):\n",
    "    '''\n",
    "    Generate a tensor using given matrices.\n",
    "    Params:\n",
    "        p_layer: matrix of past layer\n",
    "        n_layer: matrix of now layer\n",
    "        f_layer: matrix of future layer\n",
    "    Return:\n",
    "        A torch tensor.\n",
    "    '''\n",
    "    # create a snapshot\n",
    "    snapshot = np.zeros([img_size, img_size, 3], dtype='float64')\n",
    "\n",
    "    # unexpected zones\n",
    "    left_zones = set()\n",
    "\n",
    "    # future-Red: 0\n",
    "    for _, row in f_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 0] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # past-Green: 1\n",
    "    for _, row in p_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 1] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # now-Blue: 2\n",
    "    for _, row in n_layer.iterrows():\n",
    "        try:\n",
    "            snapshot[mp[str(row['pulocationid'])], mp[str(row['dolocationid'])], 2] += 1\n",
    "        except Exception as e:\n",
    "            left_zones.add(str(row['pulocationid']))\n",
    "            left_zones.add(str(row['dolocationid']))\n",
    "\n",
    "    # normalize\n",
    "    snapshot *= 255 // snapshot.max()\n",
    "    snapshot = torch.from_numpy(snapshot)\n",
    "    return snapshot\n",
    "\n",
    "\n",
    "# Function that gets a specific layer of snapshot.\n",
    "def get_channel(image, layer:str):\n",
    "    '''\n",
    "    Get a layer of the snapshot.\n",
    "    Params:\n",
    "        image: PIL image\n",
    "        channel: one of R-F,G-P,B-N\n",
    "    Return:\n",
    "        single channel image\n",
    "    '''\n",
    "    assert layer in ['P', 'N', 'F']\n",
    "    namedict = {'P': 'G', 'N': 'B', 'F': 'R'}\n",
    "    chandict = {'R':0, 'G':1, 'B':2}\n",
    "    template = np.array(image)\n",
    "    chan = np.zeros([*template.shape], dtype='uint8')\n",
    "    chan[:,:,chandict[namedict[layer]]] = image.getchannel(namedict[layer])\n",
    "    chan = Image.fromarray(chan)\n",
    "    return chan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image save directory:\n",
    "visual_path = r'D:\\GITHUB\\GSPNet\\snapshots\\visualization'\n",
    "data_path = r'D:\\GITHUB\\GSPNet\\snapshots\\data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started at Tue Dec 25 22:01:56 2018\n",
      "Generating time intervals...\n",
      "4465 4464\n",
      "Preparing table data...\n",
      "table shape: (10222693, 11)\n",
      "start generating...\n",
      "2017-03-01 00:00:00 2017-03-01 00:10:00\n",
      "2017-03-01 00:10:00 2017-03-01 00:20:00\n",
      "2017-03-01 00:20:00 2017-03-01 00:30:00\n",
      "2017-03-01 00:30:00 2017-03-01 00:40:00\n",
      "2017-03-01 00:40:00 2017-03-01 00:50:00\n",
      "2017-03-01 00:50:00 2017-03-01 01:00:00\n",
      "2017-03-01 01:00:00 2017-03-01 01:10:00\n",
      "2017-03-01 01:10:00 2017-03-01 01:20:00\n",
      "2017-03-01 01:20:00 2017-03-01 01:30:00\n",
      "2017-03-01 01:30:00 2017-03-01 01:40:00\n",
      "2017-03-01 01:40:00 2017-03-01 01:50:00\n",
      "2017-03-01 01:50:00 2017-03-01 02:00:00\n",
      "2017-03-01 02:00:00 2017-03-01 02:10:00\n",
      "2017-03-01 02:10:00 2017-03-01 02:20:00\n",
      "2017-03-01 02:20:00 2017-03-01 02:30:00\n",
      "2017-03-01 02:30:00 2017-03-01 02:40:00\n",
      "2017-03-01 02:40:00 2017-03-01 02:50:00\n",
      "2017-03-01 02:50:00 2017-03-01 03:00:00\n",
      "2017-03-01 03:00:00 2017-03-01 03:10:00\n",
      "2017-03-01 03:10:00 2017-03-01 03:20:00\n",
      "2017-03-01 03:20:00 2017-03-01 03:30:00\n",
      "2017-03-01 03:30:00 2017-03-01 03:40:00\n",
      "2017-03-01 03:40:00 2017-03-01 03:50:00\n",
      "2017-03-01 03:50:00 2017-03-01 04:00:00\n",
      "2017-03-01 04:00:00 2017-03-01 04:10:00\n",
      "2017-03-01 04:10:00 2017-03-01 04:20:00\n",
      "2017-03-01 04:20:00 2017-03-01 04:30:00\n",
      "2017-03-01 04:30:00 2017-03-01 04:40:00\n",
      "2017-03-01 04:40:00 2017-03-01 04:50:00\n",
      "2017-03-01 04:50:00 2017-03-01 05:00:00\n",
      "2017-03-01 05:00:00 2017-03-01 05:10:00\n",
      "2017-03-01 05:10:00 2017-03-01 05:20:00\n",
      "2017-03-01 05:20:00 2017-03-01 05:30:00\n",
      "2017-03-01 05:30:00 2017-03-01 05:40:00\n",
      "2017-03-01 05:40:00 2017-03-01 05:50:00\n",
      "2017-03-01 05:50:00 2017-03-01 06:00:00\n",
      "2017-03-01 06:00:00 2017-03-01 06:10:00\n",
      "2017-03-01 06:10:00 2017-03-01 06:20:00\n",
      "2017-03-01 06:20:00 2017-03-01 06:30:00\n",
      "2017-03-01 06:30:00 2017-03-01 06:40:00\n",
      "2017-03-01 06:40:00 2017-03-01 06:50:00\n",
      "2017-03-01 06:50:00 2017-03-01 07:00:00\n",
      "2017-03-01 07:00:00 2017-03-01 07:10:00\n",
      "2017-03-01 07:10:00 2017-03-01 07:20:00\n",
      "2017-03-01 07:20:00 2017-03-01 07:30:00\n",
      "2017-03-01 07:30:00 2017-03-01 07:40:00\n",
      "2017-03-01 07:40:00 2017-03-01 07:50:00\n",
      "2017-03-01 07:50:00 2017-03-01 08:00:00\n",
      "2017-03-01 08:00:00 2017-03-01 08:10:00\n",
      "2017-03-01 08:10:00 2017-03-01 08:20:00\n",
      "2017-03-01 08:20:00 2017-03-01 08:30:00\n",
      "2017-03-01 08:30:00 2017-03-01 08:40:00\n",
      "2017-03-01 08:40:00 2017-03-01 08:50:00\n",
      "2017-03-01 08:50:00 2017-03-01 09:00:00\n",
      "2017-03-01 09:00:00 2017-03-01 09:10:00\n",
      "2017-03-01 09:10:00 2017-03-01 09:20:00\n",
      "2017-03-01 09:20:00 2017-03-01 09:30:00\n",
      "2017-03-01 09:30:00 2017-03-01 09:40:00\n",
      "2017-03-01 09:40:00 2017-03-01 09:50:00\n",
      "2017-03-01 09:50:00 2017-03-01 10:00:00\n",
      "2017-03-01 10:00:00 2017-03-01 10:10:00\n",
      "2017-03-01 10:10:00 2017-03-01 10:20:00\n",
      "2017-03-01 10:20:00 2017-03-01 10:30:00\n",
      "2017-03-01 10:30:00 2017-03-01 10:40:00\n",
      "2017-03-01 10:40:00 2017-03-01 10:50:00\n",
      "2017-03-01 10:50:00 2017-03-01 11:00:00\n",
      "2017-03-01 11:00:00 2017-03-01 11:10:00\n",
      "2017-03-01 11:10:00 2017-03-01 11:20:00\n",
      "2017-03-01 11:20:00 2017-03-01 11:30:00\n",
      "2017-03-01 11:30:00 2017-03-01 11:40:00\n",
      "2017-03-01 11:40:00 2017-03-01 11:50:00\n",
      "2017-03-01 11:50:00 2017-03-01 12:00:00\n",
      "2017-03-01 12:00:00 2017-03-01 12:10:00\n",
      "2017-03-01 12:10:00 2017-03-01 12:20:00\n",
      "2017-03-01 12:20:00 2017-03-01 12:30:00\n",
      "2017-03-01 12:30:00 2017-03-01 12:40:00\n",
      "2017-03-01 12:40:00 2017-03-01 12:50:00\n",
      "2017-03-01 12:50:00 2017-03-01 13:00:00\n",
      "2017-03-01 13:00:00 2017-03-01 13:10:00\n",
      "2017-03-01 13:10:00 2017-03-01 13:20:00\n",
      "2017-03-01 13:20:00 2017-03-01 13:30:00\n",
      "2017-03-01 13:30:00 2017-03-01 13:40:00\n",
      "2017-03-01 13:40:00 2017-03-01 13:50:00\n",
      "2017-03-01 13:50:00 2017-03-01 14:00:00\n",
      "2017-03-01 14:00:00 2017-03-01 14:10:00\n",
      "2017-03-01 14:10:00 2017-03-01 14:20:00\n",
      "2017-03-01 14:20:00 2017-03-01 14:30:00\n",
      "2017-03-01 14:30:00 2017-03-01 14:40:00\n",
      "2017-03-01 14:40:00 2017-03-01 14:50:00\n",
      "2017-03-01 14:50:00 2017-03-01 15:00:00\n",
      "2017-03-01 15:00:00 2017-03-01 15:10:00\n",
      "2017-03-01 15:10:00 2017-03-01 15:20:00\n",
      "2017-03-01 15:20:00 2017-03-01 15:30:00\n",
      "2017-03-01 15:30:00 2017-03-01 15:40:00\n",
      "2017-03-01 15:40:00 2017-03-01 15:50:00\n",
      "2017-03-01 15:50:00 2017-03-01 16:00:00\n",
      "2017-03-01 16:00:00 2017-03-01 16:10:00\n",
      "2017-03-01 16:10:00 2017-03-01 16:20:00\n",
      "2017-03-01 16:20:00 2017-03-01 16:30:00\n",
      "2017-03-01 16:30:00 2017-03-01 16:40:00\n",
      "2017-03-01 16:40:00 2017-03-01 16:50:00\n",
      "2017-03-01 16:50:00 2017-03-01 17:00:00\n",
      "2017-03-01 17:00:00 2017-03-01 17:10:00\n",
      "2017-03-01 17:10:00 2017-03-01 17:20:00\n",
      "2017-03-01 17:20:00 2017-03-01 17:30:00\n",
      "2017-03-01 17:30:00 2017-03-01 17:40:00\n",
      "2017-03-01 17:40:00 2017-03-01 17:50:00\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(f'Process started at {time.ctime()}')\n",
    "print(f'Generating time intervals...')\n",
    "timelist = timesplit('2017-03-01 00:00:00', '2017-04-01 00:00:00')\n",
    "# convert dtype for entire table here:\n",
    "print(f'Preparing table data...')\n",
    "table = pd.read_csv('dataset/nytaxi_yellow_2017_mar.csv')\n",
    "print(f'table shape: {table.shape}')\n",
    "table['tpep_pickup_datetime'] = pd.to_datetime(table['tpep_pickup_datetime'])\n",
    "table['tpep_dropoff_datetime'] = pd.to_datetime(table['tpep_dropoff_datetime'])\n",
    "print('start generating...')\n",
    "for i, bound in enumerate(timelist):\n",
    "    p_layer, n_layer, f_layer = gen_snap_layers(table, bound)\n",
    "    tensor = gen_tensor(p_layer, n_layer, f_layer)\n",
    "    torch.save(tensor, f'{data_path}\\\\nytaxi_yellow_2017_mar_d{i}.pt')\n",
    "    image = gen_image(p_layer, n_layer, f_layer)\n",
    "    vimage = image.resize((690,690)) # multiply by factor of 100\n",
    "    vimage.save(f'{visual_path}\\\\nytaxi_yellow_2017_mar_v{i}.jpg')\n",
    "print(f'Image and tensor generation done in {time.time() - start :.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
